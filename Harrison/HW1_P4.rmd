---
title: "HW1_P4"
author: "Harrison Snell"
date: "2/9/2022"
output: md_document
---

## Problem 4

In this problem, we want to find the optimal value of K in our KNN regressions for each trim. We take the cross validation approach using 5 folds. In the chart of RMSE versus K for each trim there is an average RMSE mapped with one standard error bar. In each case, we use the "1SE" rule to determine our optimal K. That is we chose the largest K value that was within one standard error of the lowest average RMSE observed. Doing this with the 350 trims yields the following results.

```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
library(tidyverse)
library(mosaic)
library(ggplot2)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)

sclass <- read.csv("~/School/University of Texas-Austin/Classes/Data Mining/Data/sclass.csv")

sclass_clean = sclass %>%
  select(trim, mileage, price)%>%
  filter(trim=='65 AMG' | trim=='350')

K_folds = 5
k_grid = c(2:30,35,40,45,50,55,60,65,70,80, 90,100)

```


```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

sclass_350 = sclass_clean%>%
  filter(trim=='350')

sclass350_folds = crossv_kfold(sclass_350, k=K_folds)

cv_grid_350 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(sclass350_folds$train, ~ knnreg(price ~ mileage, k=k, data = ., use.all=FALSE))
  errs = map2_dbl(models, sclass350_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

ggplot(cv_grid_350) + 
  geom_point(aes(x=k, y=err))+
  scale_x_log10()+
  geom_errorbar(aes(x=k, ymin = err-std_err, ymax = err+std_err)) +
  labs(title = "350 Trim: RMSE for Different K Values\n", x = "K", y = "RMSE") +
  theme_classic()

opt_350 = cv_grid_350%>%
  arrange(err)

max_err_350 = opt_350[1,2] + opt_350[1,3]
min_k_350 = opt_350[1,1]

opt_350 = opt_350%>%
  filter(k >= min_k_350)%>%
  mutate(diff = max_err_350 - err)%>%
  filter(diff > 0)%>%
  arrange(diff)

optk_350 = opt_350[1,1]

sclass350_split = initial_split(sclass_350, prop = 0.8)
sclass350_train = training(sclass350_split)
sclass350_test = testing(sclass350_split)

knn350_optk <- knnreg(price ~ mileage, data=sclass350_train, k=optk_350)

sclass_350_test = sclass350_test %>%
  mutate(price_pred = predict(knn350_optk, sclass350_test))

ggplot(data = sclass_350_test) +
  theme_classic()+
  ggtitle(paste("350 Trim: Line of Fit for K=",optk_350, sep = ""))+
  geom_point(mapping = aes(x = mileage, y = price), alpha=0.2) + 
  geom_line(aes(x = mileage, y = price_pred), color='red', size=1.5)+
  scale_x_continuous(labels = scales::comma)


```



From the 350 Trim chart of RMSE versus K, we see that the optimal K is `r optk_350`, as such we fit the KNN model with k=`r optk_350` to the test data as shown above. 

Similarly, we can find an optimal K value for the AMG trim cars.


```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

sclass_AMG = sclass_clean%>%
  filter(trim=='65 AMG')

sclassAMG_folds = crossv_kfold(sclass_AMG, k=K_folds)

cv_grid_AMG = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(sclassAMG_folds$train, ~ knnreg(price ~ mileage, k=k, data = ., use.all=FALSE))
  errs = map2_dbl(models, sclassAMG_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

ggplot(cv_grid_AMG) + 
  geom_point(aes(x=k, y=err))+
  scale_x_log10()+
  geom_errorbar(aes(x=k, ymin = err-std_err, ymax = err+std_err)) +
  labs(title = "AMG Trim: RMSE for Different K Values\n", x = "K", y = "RMSE") +
  theme_classic()

opt_AMG = cv_grid_AMG%>%
  arrange(err)

max_err_AMG = opt_AMG[1,2] + opt_AMG[1,3]
min_k_AMG = opt_AMG[1,1]

opt_AMG = opt_AMG%>%
  filter(k >= min_k_AMG)%>%
  mutate(diff = max_err_AMG - err)%>%
  filter(diff > 0)%>%
  arrange(diff)

optk_AMG = opt_AMG[1,1]

sclassAMG_split = initial_split(sclass_AMG, prop = 0.8)
sclassAMG_train = training(sclassAMG_split)
sclassAMG_test = testing(sclassAMG_split)

knnAMG_optk <- knnreg(price ~ mileage, data=sclassAMG_train, k=optk_AMG)

sclass_AMG_test = sclassAMG_test %>%
  mutate(price_pred = predict(knnAMG_optk, sclassAMG_test))

ggplot(data = sclass_AMG_test) +
  theme_classic()+
  ggtitle(paste("AMG Trim: Line of Fit for K=",optk_AMG, sep = ""))+
  geom_point(mapping = aes(x = mileage, y = price), alpha=0.2) + 
  geom_line(aes(x = mileage, y = price_pred), color='red', size=1.5)


```


We see that the 1SE approach yields an optimal k=`r optk_AMG`. We observe higher optimal K values for the 350 trim as compared to the AMG. This is due to the fact that the number of observations for the AMG trim is much lower than that of the 350. Thus to earn a lower RMSE, we need a more flexible model, inducing a lower optimal K. Having a lower K will mean that the values in our neighborhood are closer to the original point, which yields a better result for the AMG trim, as the lack of observations implies that a large neighborhood could capture far away points.
