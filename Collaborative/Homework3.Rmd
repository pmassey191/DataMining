---
title: "Homework 3"
author: "Patrick Massey, Harrison Snell, Brandon Williams"
date: "4/6/2022"
output: md_document
---

```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

library(tidyverse)
library(mosaic)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(gamlr)
library(here)
library(stringr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(pdp)
library(here)
library(ggmap)
```

# Homework 3


## Problem 1

1.	The problem with a simple regression of "crime" on "police" is that crime is endogenous in police. That is, more crime might directly cause more police (a particularly high crime area gets more police, instead of the reverse, undermining any element of causality). Therefore, we can’t so easily identify the effect of policing on crime. 
2.	The researchers used a unique element of Washington, DC, where extra police are mandated by the national “terror alert” system. On high alert days, more police are automatically put on the street. Therefore, police presence is not a function of “street” crime and is entirely determined exogenously. The results indicate that extra police presence as a result of high alert is associated with a decrease in crime when police presence is determined by something other than the local crime level. This is useful evidence in the case for the deterrent effect of police on crime. 
3.	Ridership controls for a decrease in crime caused by fewer people (fewer tourists on a given day, for example) in the area because they are concerned about a high terrorism alert. In other words, this provides a check on the hypothesis that the decrease in crime is related to the increased police presence, and not a decrease in civilian and criminal activity directly related to concerns about terrorism itself. 
4.	This model appears to be modeling the interaction between the high alert variable and the various police districts of DC, while controlling for metro ridership. The results indicate that midday ridership is associated with an increase in crime, and that high alert status in the first police district is associated with a decrease in crime. However, this effect seems to lessen or disappear in the other districts (not district 1), and the coefficient on the interaction variable in any other district is not statistically distinguishable from zero. It is likely this was included because it allows the researchers to look where police concentration is largest (the first district) while holding other confounders constant (such as weather, unrelated changes in tourism, etc.) that should effect the districts similarly. 


## Problem 2

Our model will consider a prediction of log cases of dengue using CART, random forest, and boosted trees. Our model will consider `city`, `season`, `precipitation_amt`, `avg_temp_k`, `air_temp_k`, `dew_point_temp_k`, `max_air_temp_k`,`min_air_temp_k`, `precip_amt_kg_per_m2`, `relative_humidity_percent`, `specific_humidity`, and `tdtr_k` as features. 

Let's begin by taking a look at our CART. 

```{r include=FALSE}

dengue <- read_csv(here("data/dengue.csv"))

# To avoid errors in partial plot, we need to clean the data a bit, and also convert to log cases.

dengue <- dengue %>% 
  mutate(log_cases = log(total_cases+1)) %>% 
  as.data.frame() %>% 
  na.omit()

dengue$season = as.factor(dengue$season)
dengue$city = as.factor(dengue$city)

# let's split our data into training and testing sets

set.seed(1833)
dengue_split =  initial_split(dengue, prop=0.8)
set.seed(1833)
dengue_train = training(dengue_split)
set.seed(1833)
dengue_test  = testing(dengue_split)

```


```{r, echo = FALSE, results = FALSE, warning = FALSE, message = FALSE}

## A tree with CP set to .005 and minimum in each split at 30

dengue.tree = rpart(log_cases ~ city + season + precipitation_amt + avg_temp_k + air_temp_k + dew_point_temp_k + max_air_temp_k + min_air_temp_k + precip_amt_kg_per_m2 + relative_humidity_percent + specific_humidity + tdtr_k, 
                    data=dengue_train,
                    control = rpart.control(cp = .005, minsplit = 30))

rpart.plot(dengue.tree, type=4, extra=1)
```

We are already starting to lose some interpretability. What if we use the 1SE rule to prune the tree back?

```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

# Let's prune our tree at the 1se level

prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}

dengue.tree_prune = prune_1se(dengue.tree)

rpart.plot(dengue.tree_prune, type=4, extra=1)

```

Much more parsimonious. Our first split comes from Average Diurnal Temperature Range (DTR), and then our splits come from season and specific humidity. 

Let's consider a random forest instead. Here you can see MSE as a function of the number of trees used in the random forest. It levels out before reaching 100 trees, so we aren't gaining that much after that. 

```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

# Random Forest

dengue.forest = randomForest(log_cases ~ city + season + precipitation_amt + avg_temp_k + air_temp_k + dew_point_temp_k + max_air_temp_k + min_air_temp_k + precip_amt_kg_per_m2 + relative_humidity_percent + specific_humidity + tdtr_k,
                             data=dengue_train, importance = TRUE, na.action=na.omit)

# shows out-of-bag MSE as a function of the number of trees used
plot(dengue.forest)

```
``` {r include=FALSE}
modelr::rmse(dengue.tree_prune, dengue_test)
modelr::rmse(dengue.forest, dengue_test)
```

Let's compare RMSEs to see how we are doing. The pruned tree has an RMSE of .9372, while the random forest has an RMSE of .8767, a marked improvement (recall that our predictions are in log form).

What about variable importance? Is there anything that could be ignored from the random forest without noticeable improvement?

```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
# variable importance measures
vi = varImpPlot(dengue.forest, type=1)


```

Precipitation amount has the lowest variable importance, probably because it is partially captured by precipitation per square meter, but it is also not irrelevant. 

Finally, let's consider a gradient boosted tree. To compare different the different tuning parameters, we create a for-loop that runs through a number of interaction depth, shrinkage, and number of tree variables. Seeing the lowest RMSE value allows us to set the parameters as optimal. 

``` {r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

# create hyperparameter grid
# hyper_grid <- expand.grid(
#   shrinkage = c(.001, .003, .005, .008, .01),
#   interaction.depth = c(3, 4, 5, 6, 7),
#   optimal_trees = 0,               # a place to dump results
#   min_RMSE = 0                     # a place to dump results
# )
# 
# # total number of combinations
# ## [1] 25
# 
# for(i in 1:nrow(hyper_grid)) {
# 
#   # reproducibility
#   set.seed(1833)
# 
#   # train model
#   dengue.tune <- gbm(
#     formula = log_cases ~ city + season + precipitation_amt + avg_temp_k + air_temp_k + dew_point_temp_k + max_air_temp_k + min_air_temp_k + precip_amt_kg_per_m2 + relative_humidity_percent + specific_humidity + tdtr_k,
#     distribution = "gaussian",
#     data = dengue_train,
#     n.trees = 10000,
#     interaction.depth = hyper_grid$interaction.depth[i],
#     shrinkage = hyper_grid$shrinkage[i],
#     train.fraction = .75,
#     n.cores = NULL, # will use all cores by default
#     verbose = FALSE
#   )
# 
#   # add min training error and trees to grid
#   hyper_grid$optimal_trees[i] <- which.min(dengue.tune$valid.error)
#   hyper_grid$min_RMSE[i] <- sqrt(min(dengue.tune$valid.error))
# }
# 
# hyper_grid %>%
#   dplyr::arrange(min_RMSE) %>%
#   head(10)
```

Let's set the shrinkage to .01 and the interaction depth to 4. Trees will be set to 500 since it converged around the mid-400s. 

```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

dengue.boost = gbm(log_cases ~ city + season + precipitation_amt + avg_temp_k + air_temp_k + dew_point_temp_k + max_air_temp_k + min_air_temp_k + precip_amt_kg_per_m2 + relative_humidity_percent + specific_humidity + tdtr_k,
             data=dengue_train,
             interaction.depth=4, n.trees=500, shrinkage=.01)


```
Comparing RMSEs, the random forest does just slightly better than the boosted model. 

``` {r}

# RMSE comparison
modelr::rmse(dengue.tree, dengue_test)
modelr::rmse(dengue.tree_prune, dengue_test)
modelr::rmse(dengue.forest, dengue_test)
modelr::rmse(dengue.boost, dengue_test)

```


Let's look at partial dependence plots from the boosted tree. We include `specific_humidity` and `precipitation_amt`, as well as `tdtr_k` since it played an important role in the first tree we looked at. 

```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
# Partial plots

#partialPlot(dengue.forest, dengue_test, 'specific_humidity', las=1)
#partialPlot(dengue.boost, dengue_test, 'precipitation_amt', las=1)
#partialPlot(dengue.boost, dengue_test, 'tdtr_k', las=1)
```

##Green Certification

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(rsample) 
library(randomForest)
library(modelr)
library(caret)
library(gbm)
library(knitr)
greenbuildings <- read.csv("../data/greenbuildings.csv")%>%
   mutate(revenue = Rent*(leasing_rate/100),
           utility_cost = net*Gas_Costs + net*Electricity_Costs) %>%
   select(-empl_gr)
greenbuildings_split <- initial_split(greenbuildings,.8)
greenbuildings_test <- testing(greenbuildings_split)
greenbuildings_train <- training(greenbuildings_split)
```

Before developing any models we first begin by performing some feature engineering. The first feature we engineer is the outcome variable of interest revenue which represents the revenue per square foot per calendar year. In order to create this feature we first scale down leasing_rate to a percentage by dividing by 100, and then multiplying that by the rent. We also create a new feature called utility_cost which is the sum of gas and electricity costs for rents that are quoted on a net contract basis. The purpose of this new feature is to capture the costs associated with a rental offered on a net contract basis. We then create a training set and a testing set with a split of 80/20. This gives us 6315 observations in our training set and 1579 observations in our testing set.

To begin developing our model we start with a linear model using all features of the data set excluding, CS_PropertyID, cluster, leasing_rate, Rent, LEED, and Energystar. We remove CS_PropertyID as it is just a unique building ID, and for similar reasons we remove cluster. We remove leasing_rate and Rent since these variables directly calculate our outcome variable. Lastly we remove LEED and Energystar because we are only concerned if a building is green certified or not, and not what kind of green certification a building may have. We capture this with the green_rating feature. 
```{r, echo=FALSE, warning=FALSE }
lm_base = lm(revenue ~ . - CS_PropertyID - cluster-leasing_rate-Rent-LEED-Energystar, data = greenbuildings_train)
```


After getting a baseline model we then moved onto predicting using a tree model. The initial tree model generated, shown below, was extremely complex and not readable. This indicated that there might be some overfitting happening.

```{r,echo=FALSE,warning=FALSE }
gb_tree = rpart(revenue ~ . - CS_PropertyID - cluster-leasing_rate-Rent-LEED-Energystar,
                  data=greenbuildings_train, control = rpart.control(cp = 0.0001,minsplit = 30))
rpart.plot(gb_tree, digits=-5, type=4, extra=1)
```

 We then pruned our tree using the 1se method which generated the much simpler decision tree shown below. This tree sacrifices a marginal amount of performance for a much simpler tree.
 
```{r,echo=FALSE,warning=FALSE }
prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}
gb_tree_prune = prune_1se(gb_tree)
rpart.plot(gb_tree_prune, digits=-5, type=4, extra=1)

```

The visualization of the tree really highlighted the interactions that were not included in our baseline linear model. Naturally after seeing the performance of the tree as compared to the linear model we wanted to see if it could be improved upon using a random forest.

```{r, echo=FALSE,warning=FALSE}
gb_forest = randomForest(revenue ~ . - CS_PropertyID - cluster-leasing_rate-Rent-LEED-Energystar,
                           data=greenbuildings_train, importance = TRUE)
plot(gb_forest)
```

We see that our error really starts to bottom out around 100 trees. The performance of our models is shown below.

```{r,echo=FALSE,warning=FALSE }
Model <- c("Linear", "CART", "Pruned Tree", "Random Forest")
RMSE <- c(modelr::rmse(lm_base , greenbuildings_test), modelr::rmse(gb_tree, greenbuildings_test), modelr::rmse(gb_tree_prune, greenbuildings_test), modelr::rmse(gb_forest, greenbuildings_test))
df <- data.frame(Model, RMSE)
kable(df)
```
The random forest provides a significant reduction in RMSE as compared to our baseline linear model. 

Now that we have developed a model for predicting the revenue generated from an building we will look at the importance of the variables we have used in our model.

```{r, echo=FALSE,warning=FALSE }
varImpPlot(gb_forest, main = "Variable Importance Plot")
```

We can see that from a prediction point of view the green rating of a building does provide a large (>10%) increase in RMSE performance. Now lets look at the dollar increase in revenue from a building that has a green rating by creating a partial dependence plot shown below.

```{r, echo=FALSE,warning=FALSE }
partialPlot(gb_forest, greenbuildings_test, 'green_rating', las=1)
```

From the plot we can see that there is a small marginal improvement in the expected revenue for a green building versus a non-green building. 



## California Housing

The goal of this problem is to build the best possible model for predicting the median house value. We start by building the best possible linear, KNN, CART, Random Forest, and Boosted models. After the initial train-test split, we use just the training data to again split for each models building and optimization process.

```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
# Loading Data
CAhousing <- read_csv(here("data/CAhousing.csv"))

# Adding averages for rooms and bedrooms
CAhousing = CAhousing%>%
  mutate(avg_room = totalRooms/households,
         avg_bed = totalBedrooms/households)

# Initial train-test split
CAhousing_split = initial_split(CAhousing,0.8)
ca_train = training(CAhousing_split)
ca_test = testing(CAhousing_split)
```

The first step for each model is to make a new train-test split from our global train-test split. Then use these new sets to build and test iterations of different models.


```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
##### Linear Model

ca_linear_folds = crossv_kfold(ca_train, k=5)

linear_models = map(ca_linear_folds$train, ~ lm(medianHouseValue ~. - totalBedrooms - totalRooms + medianIncome*housingMedianAge + longitude*latitude, data = ., ))
linear_errs = map2_dbl(linear_models, ca_linear_folds$test, modelr::rmse)
linear_errs = mean(linear_errs)

##### KNN Model
ca_knn_split = initial_split(ca_train,.8)
ca_knn_train = training(ca_knn_split)
ca_knn_test = testing(ca_knn_split)

X = ca_knn_train[,-(10:11)]
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
test = ca_knn_test[,-(10:11)]
Y = (test[,]-mu)/sigma

ca_knn = knnreg(medianHouseValue ~ ., data = X, k=15)
knn_errs = rmse(ca_knn, Y)

#### CART Model

prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}

ca_cart_folds = crossv_kfold(ca_train, k=5)

cart_models = map(ca_cart_folds$train, ~prune_1se(rpart(medianHouseValue ~ .- totalRooms - totalBedrooms,
                                             data =., 
                                             control = rpart.control(cp = .0001, minsplit = 10))))
cart_errs = map2_dbl(cart_models, ca_cart_folds$test, modelr::rmse)
cart_errs = mean(cart_errs)


#### Random Forest

ca_forest_split = initial_split(ca_train, 0.8)
ca_forest_train = training(ca_forest_split)
ca_forest_test = testing(ca_forest_split)

ca_forest = randomForest(medianHouseValue ~ . - totalBedrooms - totalRooms,
                         data = ca_forest_train,
                         importance = TRUE)
forest_errs = rmse(ca_forest, ca_forest_test)

#### Boosting
ca_boost_folds = crossv_kfold(ca_train, k=5)

boost_models = map(ca_boost_folds$train, ~ gbm(medianHouseValue ~ . - totalRooms - totalBedrooms,
                                               data =.,
                                               interaction.depth = 6, n.trees = 600, shrinkage = 0.05))
boost_errs = map2_dbl(boost_models, ca_boost_folds$test, modelr::rmse)
boost_errs = mean(boost_errs)
```

We can see that the best model from each category has an out-of-sample error based on their own train-test splits. For example, we have an average RMSE from the linear model of `linear_errs` or an average RMSE of `boost_errs` from the boosted model. However, these RMSE's are from train-test splits built from the original train-test split. In order to compare RMSE's across model categories, we return to our first train-test split to recover RMSE's and determine the best predictive model.

```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
#### Comparing to full train-test split

# Linear
final_linear = lm(medianHouseValue ~. - totalBedrooms - totalRooms + medianIncome*housingMedianAge + longitude*latitude,
                  data = ca_train)
linear_rmse = rmse(final_linear, data = ca_test)

# KNN

X = ca_train[,-(10:11)]
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
Y = ca_test[,-(10:11)]
Y = (Y[,]-mu)/sigma

final_knn = knnreg(medianHouseValue ~ ., data = X, k=15)
knn_rmse = rmse(final_knn, Y)

# CART

final_cart = rpart(medianHouseValue ~ .- totalRooms - totalBedrooms, data =ca_train, 
                   control = rpart.control(cp = .0001, minsplit = 10))
final_cart = prune_1se(final_cart)

cart_rmse = rmse(final_cart, data=ca_test)

# Random Forest

final_forest = randomForest(medianHouseValue ~ . - totalBedrooms - totalRooms,
                         data = ca_train,
                         importance = TRUE)
forest_rmse = rmse(final_forest, ca_test)


# Boosting

final_boost = gbm(medianHouseValue ~ . - totalRooms - totalBedrooms,
                  data =ca_train,
                  interaction.depth = 6, n.trees = 600, shrinkage = 0.05)
boost_rmse = rmse(final_boost, ca_test)
```

Comparing out-of-sample errors across models, we see that the boosting model and the random forest perform quite similarly, with RMSE's of `boost_rmse` and `forest_rmse` respectively. This is compared to something like the linear model which yields `linear_rmse` and we see that both the boosting and random forets are considerable improvements. The CART and KNN models fall somewhere in between and we include their RMSE values for completness, CART has an RMSE of `cart_rmse` and KNN yields `knn_rmse`. Finally, we will move forward with the boosting as the best predictive model and create our plots.



```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
#### Plots for winner

ca_winner = final_boost

test = CAhousing%>%
  mutate(predictedHouseValue = predict(ca_winner, CAhousing))%>%
  mutate(Residuals = medianHouseValue - predictedHouseValue)

qmplot(longitude, latitude, color = medianHouseValue,
       data = CAhousing)+
  scale_color_continuous(type = 'viridis')

qmplot(longitude, latitude, color = predictedHouseValue,
       data = test)+
  scale_color_continuous(type = 'viridis')

qmplot(longitude, latitude, color = Residuals,
       data = test)+
  #scale_color_gradient2(low = 'black',mid = 'red')
  scale_color_continuous(type = 'viridis')
```
