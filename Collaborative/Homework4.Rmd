---
title: "Homework 4"
author: "Patrick Massey, Harrison Snell, Brandon Williams"
date: "5/2/2022"
output: md_document
---

```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
options(scipen=999)
library(tidyverse)
library(mosaic)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(gamlr)
library(here)
library(stringr)
library(rpart)
library(rpart.plot)
library(randomForest)
library(gbm)
library(pdp)
library(here)
library(ggmap)
library(knitr)
library(LICORS)  
library(arules)  
library(arulesViz)
library(igraph)
```

# Homework 4

Patrick Massey, Harrison Snell, Brandon Williams

## Problem 1 - Wine Clustering and PCA

In this problem, we want to use a clustering approach and a PCA approach to use the chemical properties of wine to determine the color of the wine. We will start by running K-means++ as our clustering algorithm with 2 clusters. We ran clustering with more than 2 clusters to see if any larger trends emerged. Specifically, in the realm of quality, however clustering was not able to capture any useful divisions of quality given the chemical properties of the wine. The same goes for PCA with none of the principal components picking up on the quality of the wine. This is not a surprise as it is well known that even wine experts tend to not be able to accurately judge the quality of a wine. For PCA, we choose to run the analysis with rank 6 to capture a large amount of the variability. In both cases, the chemical properties of the wine were scaled and centered.


```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
wine <- read.csv(here("Data/wine.csv"))

# Clustering Approach
# Scaling properties of interest
X = wine[,-c(1,13)]
X = scale(X, center=TRUE, scale=TRUE)

# Running Clustering Algorithm
clust_kplus = kmeanspp(X, k=2, nstart=25)

# Confusion Matrix of Clustering Results
con_matrix = table(wine$color,factor(clust_kplus$cluster))
con_matrix


```

Clustering gets about 97.4% of wines correctly identified by their color. When we compare a few charts of the chemical properties colored by either the true color of the wine or the clustering results, we see that the graphs are very similar.

```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

# Clustering plot of Density and Total Sulfur Dioxide
ggplot(wine)+
  geom_point(aes(x=density, y=total.sulfur.dioxide, color = factor(clust_kplus$cluster)))+
  ggtitle("Density and Total Sulfur Dioxide colored by Cluster")

# True plot of Density and Total Sulfur Dioxide
ggplot(wine)+
  geom_point(aes(x=density,y=total.sulfur.dioxide,color=color))+
  ggtitle("Density and Total Sulfur Dioxide colored\n by True Wine Color")

# Clustering Plot of Fixed Acidity and Chlorides

ggplot(wine)+
  geom_point(aes(x=fixed.acidity, y=chlorides, color = factor(clust_kplus$cluster)))+
  ggtitle("Fixed Acidity and Chlorides colored by Cluster")

# True Plot of Fixed Acidity and Chlorides

ggplot(wine)+
  geom_point(aes(x=fixed.acidity,y=chlorides,color=color))+
  ggtitle("Fixed Acidity and Chlorides colored by True Wine Color")


```
As we can see from the visualization, the clustering algorithm picks up well on the color of the wines. We can now look at the PCA approach.

```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

# PCA Approach
wine = wine %>%
  rownames_to_column('wine_id')

wine$wine_id = as.numeric(wine$wine_id)

pc_wine = prcomp(wine[,-c(1,13,14)], rank=6, scale=TRUE)

summary(pc_wine)

outcome = as.data.frame(pc_wine$x)

outcome = outcome %>%
  rownames_to_column('wine_id')

wine_full = merge(wine, outcome, by = 'wine_id')

wine_full = wine_full %>%
  select(!wine_id)

ggplot(wine_full)+
  geom_point(aes(x=color,y=PC1))+
  ggtitle("Wine Color and PC1")

ggplot(wine_full)+
  geom_point(aes(x=color,y=PC2))+
  ggtitle("Wine Color and PC2")

ggplot(wine_full)+
  geom_point(aes(x=PC1,y=PC2,color=color))+
  ggtitle("PC1 and PC2 colored by Wine Color")

ggplot(wine_full)+
  geom_point(aes(x=PC1,y=PC3,color=color))+
  ggtitle("PC1 and PC3 colored by Wine Color")

ggplot(wine_full)+
  geom_point(aes(x=PC2,y=PC4,color=color))+
  ggtitle("PC2 and PC4 colored by Wine Color")



```

After running PCA with rank 6, we see that we capture about 85% of the variation. PC1, with the highest proportion of that variance, is the component that best captures the color classification of the wines. In the first pair of graphs comparing color to a couple principle components, we see that the color of the wine is almost indistinguishable in the case of PC2. For a given value of PC2, it is unlikely that we would be able to tell the color. For PC1, that is a good bit of difference between the reds and the whites. This means that given a value for PC1, we are able to make a good guess as to the color of the wine. The next sets of graphs show that using PC1 and at least one other component can result in a good predictions of wine color. If we were to run a simple supervised learning algorithm with all our principle components, it would likely do a very good job identifying the color of wines given their PCA weightings. Again, we see that PC1 appears to be doing most of the heavy lifting. When we look at the graph with PC2 and PC4, we see the general result of two non-PC1 components that is a blob of points that all look the same. The fact that a single principle component seems to be providing most of the color identification lends itself to the idea that clustering is a better approach for this problem. The clustering approach is more intuitive for separating wines by their color given chemical features. Of course, running supervised learning on the principle components could also end with very accurate results, we start to run into the problem of actually reducing dimensions. Running a supervised learning algorithm with 6 components does not reduce the dimension much with our 11 chemical properties. Given that the clustering algorithm is about 94% accurate and remains the more simple and intuitive approach to separating wines into the 2 groups of color, we believe that clustering is the best unsupervised approach to this question. 



## Problem 2 - NutrientH2O

When considering marketing to the NutrientH2O Twitter followers, there are several natural groups that emerge using unsupervised learning techniques. These groups will help NutrientH20 coordinate a marketing campaign to specific follower demographics. Before conducting principle component analysis (PCA), let's take a look at the overall trends of the tweets of the Twitter followers. 

```{r echo=FALSE}
df <- read.csv(here("data/social_marketing.csv"))
df <- select(df, -X)

# some summary statistics

avg_user <- 
  as.data.frame(sort(colMeans(df)))
avg_user <- avg_user %>% 
  rownames_to_column("tweet_cat") %>% 
  rename("avg_tweets" = "sort(colMeans(df))")


ggplot(avg_user) + 
  geom_col(aes(x = reorder(tweet_cat, -avg_tweets), y = avg_tweets)) + 
  coord_flip() +
  ylab("Average Tweets Per User") +
  xlab("Tweet Type")
```

As we might have predicted, the average user generally tweets about general, uncategorized "chatter" and some photo sharing. Nevertheless, we see some high counts in health and nutrition, cooking, politics, sports, and travel. Are there natural groupings of followers into categories? Consider the correlation matrix of tweet type organized by hierarchical clustering: 

```{r pressure, echo=FALSE}

# a heatmap visualization with sorting by hierarchical clustering
ggcorrplot::ggcorrplot(cor(df), hc.order = TRUE)
```

Some patterns emerge. Disregarding the spam/adult categories (unless NutrientH2O is considering a significant rebrand), there are clear groupings around:
  * beauty, cooking, and fashion 
  * health, nutrition, and fitness
  * family, school, religion, and parenting
  * gaming, sports, and university

Since clustering is naturally mutually exclusive and Twitter users can have multiple interests simultaneously, we choose to present a PCA on the follower base to see if these categorical patterns continue to emerge based on the components of the Tweets. An initial PCA reveals how much variance is explained by each principle component. We see the variance dropping off after 5 components, and certainly after 10 we are gaining smaller and smaller amounts of variance. 

``` {r echo=F, messages = F, results = F, warning = F}

pca1 = prcomp(df, scale=TRUE) 

#calculate total variance explained by each principal component
var_explained = pca1$sdev^2 / sum(pca1$sdev^2)

#create scree plot
qplot(c(1:36), var_explained) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Variance Explained by PC") +
  ylim(0, .25)
```

Running a PCA with 10 principle components gives us some pretty clear group characteristics. Let's look at the loadings for some of the components. The first principle component is once again the adult/spam group, so let's look at the high loadings for PC2. We can call this the "influencer" group: high in cooking, fashion, shopping, and beauty. Considering the overall popularity of these types of tweets among NutrientH2O followers, this would be an effective demographic to market to.


``` {r echo=F, results = T, warning = F}

pca = prcomp(df, scale=TRUE, rank=10)


# create a tidy summary of the loadings
loadings = pca$rotation %>%
  as.data.frame() %>%
  rownames_to_column('tweet')

# This looks like the spam/adult PC
# loadings %>%
#   select(tweet, PC1) %>%
#   arrange(desc(PC1))

# Let's call this the influencer - cooking, photos, fashing, shopping
loadings %>%
  select(tweet, PC2) %>%
  arrange(desc(PC2)) %>% 
  head()

```

PC3 represents the wellness demographic--people interested in health, nutrition, personal fitness, and cooking. Given NutrientH2O's brand name, this is a natural group to target in any social media marketing. 

``` {r echo=F, results = T, warning = F}

# The wellness nut
loadings %>%
  select(tweet, PC3) %>%
  arrange(desc(PC3)) %>% 
  head()

```

Finally, PC4 shows a younger demographic, those interested in video games, sports, and college/university. The insight here for the brand is to target this market segment with branding aimed at youth and college-aged consumers. 

``` {r echo=F, results = T, warning = F}

# The college demographic - school, sports and video games
loadings %>%
  select(tweet, PC4) %>%
  arrange(desc(PC4)) %>% 
  head()

```

The other principle components begin to show less clear market segments, but some interesting ones still emerge. Consider PC7, which has high loadings in art, film, and crafts, or PC10, which seems to have a high interest in dating. 

``` {r echo=F, results = T, warning = F}
round(pca$rotation[,1:10],2)
```

Principle component analysis reveals some clear "ingredients" for each demographic, highlighting their interests, and giving valuable insight to the NutrientH2O marketing team to orient their strategy. 


## Problem 3 - Market Basket

```{r include=FALSE}
groceries_raw <- read.csv(here("Data/groceries.csv"),header = FALSE)

groceries <- groceries_raw %>% separate(V1, 
                        into =c("1","2","3","4","5","6","7","8",
                                "9","10","11","12","13","14","15","16",
                                "17","18","19","20","21","22","23","24","25",
                                "26","27","28","29","30","31","32"),
                                sep = ",")

groceries <- groceries %>% rownames_to_column(var = "customer") %>% 
  mutate(customer = as.factor(customer))

groceries <- pivot_longer(groceries,cols = !customer, 
                               names_to = "category",values_to = "grocery") %>% 
  filter(grocery != "")


groceries = split(x=groceries$grocery, f=as.factor(groceries$customer))

groceries= lapply(groceries, unique)

groc_carts = as(groceries, "transactions")
grocrules = apriori(groc_carts, 
                     parameter=list(support=.001, confidence=.05, maxlen=4))

```
Analyzing a grocery purchases is an extremely important task for grocery retailers. Understanding how products connect to certain consumers can allow the retailer to create product placement that a consumer will find useful. This creates a better experience for the consumer and helps increase sales and thus driving revenue up for the retailer. In this example we start with a data set that contains 9,835 consumer grocery baskets. The support level parameter essentially indicates the popularity of an item. When deciding the parameters we picked a support level of .001. The reasoning being that we wanted items to have appeared in approximately 10 consumers grocery carts. The confidence level is essentially the conditional probability of item x being purchased given the consumer has purchased item y. We set this level for at .05 to indicate that we wanted at least a 5% probability of a consumer purchasing one item conditional on them already purchasing another. When using these levels to create association rules we are left with 36,014 association rules. This is too many rules to make a sensible graph out of but lets look at some key plots first. 


```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
plot(grocrules, method='two-key plot')
```

In this figure we see that our rules with length four have a large variance in confidence but low support. As we shorten the rule length, there we see less variance in confidence and a higher level of support. Again our current number of rules is too much to make a sensible graph. In order to construct a graph we will subset the rules looking for a confidence greater than 25% and support greater than 1%. This leaves us with 171 rules, now lets visualize these rules with a graph.

![](Homework4_files/figure-markdown_strict/graph.png)

This graph produces some results that are both interesting and seem reasonable. The colors on the graph represent the modularity class, the size of the nodes represents the support, and the size of the text represents the degree.  We see two main groups of consumers, those who purchase whole milk and those who purchase "other vegetable". We see a slightly smaller group of consumers purchasing yogurt and root vegetables as well. 