---
title: "Homework 2"
author: "Patrick Massey, Harrison Snell, Brandon Williams"
date: "3/7/2022"
output: md_document
---

```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
options(scipen=999, digits = 0)
library(tidyverse)
library(mosaic)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(gamlr)
library(here)
library(stringr)
library(lubridate)
```

## Problem 1
```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

capmetro_UT <- read.csv(here("Data/capmetro_UT.csv"))

# Recode the categorical variables in sensible, rather than alphabetical, order
capmetro_UT = mutate(capmetro_UT,
                     day_of_week = factor(day_of_week,
                                          levels=c("Mon", "Tue", "Wed","Thu", "Fri", "Sat", "Sun")),
                     month = factor(month,
                                    levels=c("Sep", "Oct","Nov")))

fpanel = capmetro_UT %>%
  group_by(hour_of_day,day_of_week,month)%>%
  summarize(avg_board = mean(boarding))


capt = "The charts generally show that weekdays follow the same pattern of growing boardings throughout the day until a peak around rush hour. The hour of peak boarding tends to stay the same for weekdays, coming in the evening rush hour. However, weekends have a very different ridership profile that is pretty much flat throughout the day. We see that on Mondays in September, ridership is lower, this could stem from Labor Day skewing our average ridership on Mondays in September. Similarly, we also see that later week ridership is lower in November, which could stem from Thanksgiving Break being on Wednesday, Thursday, and Friday in November."

ggplot(fpanel)+
  geom_line(aes(x=hour_of_day,y=avg_board,color=month),size=1.2)+
  facet_wrap(~day_of_week)+
  theme_minimal()+
  ylab("Average Boarding")+
  xlab("Hour of the Day")+
  labs(title= "Boardings by Day of Week, Month, and Hour of Day",
       color = 'Month',
       caption = str_wrap(capt,60)) +
  theme(plot.caption = element_text(hjust = 0))
```


```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
capt2 = "In the figure, we see that ridership is consistently lower on weekends. The early mornig hours tend to have lower ridership with boardings tending to increase throughout the day until the evening. Holding hour of day and weekend/weekday fixed, the temperature does not seem to have much of an impact on boardings. Most likely people are taking the bus for class or work and thus temperature does not change the fact that people still need to get to campus or to their job. "

ggplot(capmetro_UT)+
  geom_point(aes(x=temperature,y=boarding,color=weekend),size=.5)+
  facet_wrap(~hour_of_day)+
  xlab("Temperature")+
  ylab("Boardings")+
  labs(title = "Boardings by Weekday/Weekend and Temperature faceted by Hour of Day",
       color = "Weekend",
       caption = str_wrap(capt2, 60)) +
  theme(plot.caption = element_text(hjust = 0))
```


## Problem 2

We consider three models for predicting the sale price of a house for tax authority purposes. In order to assess the predictive power of each, we conduct a "horse race" to determine who has the best predictive ability when compared to a withheld testing set of the data by evaluating root mean squared error (RMSE); generally, the lower the RMSE the better. The three models are:


  * Simple linear model (our medium benchmark model)
  * Linear model with additional features and an interaction term
  * K-Nearest Neighbor (KNN) regression
  
For all three models, we use a train/test split of the data: the training data set is used to build the model and the testing set is used to evaluate the model's performance.^1^


```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

## Load in the data and generate our folds for cross validation

data(SaratogaHouses)

K_folds = 10                                           # folds, 10 standard
k_grid = c(2:30,35,40,45,50,55,60,65,70,80,90,100)     # what values of K in KNN to check

saratoga_folds = crossv_kfold(SaratogaHouses, k=K_folds)

## Create a for loop to generate scaled data based on randomized training set

training <- foreach (i = 1:K_folds) %do% {
  
  # call each training set "train" with an index and then convert it to a matrix
  
  train_name <- paste("train_", i, sep = "") 
  train = as.data.frame(saratoga_folds$train[i])         
  train_mtx = model.matrix(~ . - 1, data=train)          
  
  # create our scale using only the training data
  
  scale_train = apply(train_mtx[,2:20], 2, sd)  # calculate std dev for each column
  
  # scale the training data using std dev of the training data
  
  scaled_train = scale(train_mtx[,2:20], scale = scale_train) %>% 
    as.data.frame()

  # this automatically removes price (because we don't want to scale it)
  # so we add it back in
  
  scaled_train = scaled_train %>%
    mutate(price = train_mtx[,1])
  
  # assign the name we created at the beginning  
  # drop prefixes and spaces in column names

  colnames(scaled_train) <- sub(".*\\.", "", colnames(scaled_train))
  names(scaled_train) <- str_replace_all(names(scaled_train), c(" " = "." , "/" = "." ))
  
  assign(train_name, scaled_train)

}

testing <- foreach (i = 1:K_folds) %do% {
  
  # repeat the same process for testing
  
  test_name <- paste("test_", i, sep = "")
  test = as.data.frame(saratoga_folds$test[i])
  test_mtx = model.matrix(~ . - 1, data=test) 
  
  # scale the testing data using std dev of the training data
  
  scaled_test = scale(test_mtx[,2:20], scale = scale_train) %>% 
    as.data.frame()
  
  # add price column back
  
  scaled_test = scaled_test %>%
    mutate(price = test_mtx[,1])
  
  # assign the name we created at the beginning  
  # drop prefixes and spaces in column names
  
  colnames(scaled_test) <- sub(".*\\.", "", colnames(scaled_test))
  names(scaled_test) <- str_replace_all(names(scaled_test), c(" " = "." , "/" = "." ))
  
  assign(test_name, scaled_test)
  
}

# We now have k-fold testing and training splits

# We can apply these to both KNN and the linear models

#####
# Medium model
#####

# baseline medium model with 11 main effects

baseline_rmse = foreach(i = 1:K_folds, .combine='rbind') %dopar% {
  
  # pull each fold and clean up
  
  test = as.data.frame(saratoga_folds$test[i])
  train = as.data.frame(saratoga_folds$train[i])
  colnames(test) <- sub(".*\\.", "", colnames(test)) 
  colnames(train) <- sub(".*\\.", "", colnames(train)) 

  # calculate rmse across all folds
  
  lm_baseline = lm(price ~ . - pctCollege - sewer - waterfront - landValue - newConstruction, data=train)
  rmse(lm_baseline, test)
}

avg_rmse_baseline <- mean(baseline_rmse)

#####
# Better linear model
#####

better_rmse = foreach(i = 1:K_folds, .combine='rbind') %dopar% {
  
  # pull each fold and clean up
  
  test = as.data.frame(saratoga_folds$test[i])
  train = as.data.frame(saratoga_folds$train[i])
  
  colnames(test) <- sub(".*\\.", "", colnames(test)) 
  colnames(train) <- sub(".*\\.", "", colnames(train)) 
  
  # calculate rmse across all folds
    
  lm_better = lm(price ~ . - pctCollege - sewer - heating + lotSize * waterfront, data=train)
  rmse(lm_better, test)
}

avg_rmse_better <- mean(better_rmse)

```

Consider first the baseline model. This model uses 11 main effects from the data set in a linear regression. It includes the variables lot size, age, living area, bedrooms, fireplaces, bathrooms, rooms, heating method, fuel method, and central air. This model performed consistently the worst. In this iteration, for example, it achieved an average out-of-sample mean-squared error of `r avg_rmse_baseline`.

This is to be expected. Economic intuition indicates that we are likely omitting important considerations for house prices, notably land value, waterfront access and whether or not it is a new construction. We add these to our linear model to improve it, as well as an interaction term for lot size and waterfront access.^2^ Indeed, we see significant improvement in the RMSE. In this iteration, we see a mean-squared error of `r avg_rmse_better`.

Finally, we attempt to create a KNN model. To begin, we include all possible covariates and attempt to identify the value of K neighbors that gives us the lowest mean-squared error. The following graph shows the error on the vertical access and the value of K on the horizontal.

```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

#####
# KNN
#####

# Calculate RMSE over folds for each value of k

cv_grid = foreach(k = k_grid, .combine='rbind') %dopar% {
  
  models = map(training, ~ knnreg(price ~ lotSize + age +landValue + livingArea +pctCollege + bedrooms + fireplaces + bathrooms + rooms + heatinghot.water.steam + heatinghot.water.steam + heatingelectric + fuelelectric + centralAirNo + fueloil + sewerpublic.commercial + sewernone + waterfrontNo + newConstructionNo, k=k, data = ., use.all=FALSE))
  
  errs = map2_dbl(models, testing, modelr::rmse)
  c(k=k, err = mean(errs), sd = sd(errs), std_err = sd(errs)/sqrt(K_folds))
  
} %>% as.data.frame

# Plot the results

ggplot(cv_grid) + 
  geom_point(aes(x=k, y=err)) + 
  geom_errorbar(aes(x=k, ymin = err-std_err, ymax = err+std_err)) + 
  scale_x_log10() +
  ylab("Mean-Squared Error")

opt_k = cv_grid%>%
  arrange(err)
min_k = opt_k[1,1]
min_rmse = opt_k[1,2]
```


The minimum RMSE^3^ can be found at k=`r min_k` with a RMSE of `r min_rmse`. Consistently, across many iterations of train/test splits, the KNN model had lower mean-squared errors than our baseline medium model but failed to beat the better linear model. It appears that the data available to us is better suited to the linear model we identified above. It is possible the KNN model is over-fitting to the data and therefore inducing higher RMSEs. We would therefore suggest pursuing a tax policy that uses the improved linear model we proposed above. 


### Footnotes
^1^ In order to account for random variation in the data depending on how we split it, we use k-fold cross-validation which takes k number (in this case, k=10) train/test splits and allows us to examine the average error over each split.

^2^ The complete model is `price` regressed on `lotSize`, `age`, `landValue`, `livingArea`, `bedrooms`, `fireplaces`, `bathrooms`, `room`, `fuel`, and an interaction term of `waterfront` * `lotSize`.

^3^ In order to truly compare lowest RMSE, we opt for choosing the K that corresponds to the lowest RMSE, instead of choosing the highest K within one standard error of the minimum (1SE method), which could be argued for in this context. 


## Problem 3
```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
german_credit <- read.csv(here("Data/german_credit.csv"))

prob_default = german_credit%>% 
  group_by(history) %>% 
  summarize(prob = mean(Default))

ggplot(prob_default)+
  geom_col(aes(x=history,y=prob),color='dark green',fill='dark green')+
  theme_minimal()+
  ggtitle("Probability of Default by Credit History")+
  ylab("Probability of Default")+
  xlab("Credit History")

german_split = initial_split(german_credit, prop = 0.8)
german_train = training(german_split)
german_test = testing(german_split)


logit_credit = glm(Default ~ (duration + amount + installment + age + 
                     history + purpose + foreign)^2, data=german_train,
                   family='binomial')

# Confusion Matrix
phat_logit_credit = predict(logit_credit,german_test,
                            type='response')
yhat_logit_credit = ifelse(phat_logit_credit >0.5 , 1 , 0)
confusion_logit = table(y = german_test$Default ,
                        yhat = yhat_logit_credit)


```


Looking at the bar plot of default probability, we see that the highest risk of default falls with those who have "good" credit histories. This is a surprise. We would expect that those with good credit history would pay back their loans, but this points to the problem with the sampling method used. By using retrospective sampling, the bank has collected a sample that overstates the probability of default for those with good credit. In predicting default with the logit model, exponentiating our coefficients correspond to odds of defaults, $e^\lambda$. In looking at the history variable, "terrible" and "poor" both have about a -3 coefficient. This implies that having a terrible or poor credit history decreases the odds of default by about $\frac{1}{e^\lambda}$. This means that good history actually increases the odds of default, an outcome of the flawed data collection. With the problems in the good credit history, the data is not adequate for a predictive model of defaults. Consider two identical people with only credit history differing, our model would say that the person with good credit history is more likely to default and thus a riskier loan. Our experience tells us that this is wrong, we would expect the person with good credit history to be more likely to pay back their loan. Since our predictive model would be poor using this retrospective sampling approach, it would be better to use a random sample of loans. Though we will see a lot of loans that are not defaulted on, it will be better to have a random sample of data to create a predictive model. That way, we have a model that matches our intuition, importantly on the fact that those with good credit history have a low risk of default.

## Problem 4
```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
options(scipen=999, digits = 4)
hotels_dev = read.csv(here('Data/hotels_dev.csv')) %>% mutate(arrival_date = ymd(arrival_date)) %>% select(-reserved_room_type)
hotels_val = read.csv(here('Data/hotels_val.csv'))%>% mutate(arrival_date = ymd(arrival_date)) %>%  select(-reserved_room_type)

#Train-Test Split
hotels_split <- initial_split(hotels_dev,.8)
hotels_train <- training(hotels_split)
hotels_test <- testing(hotels_split)

model_small <- lm(children ~ market_segment + adults + customer_type + 
                    is_repeated_guest, data = hotels_train)
model_large <- lm(children ~ . - arrival_date, data = hotels_train)

#creating train and test model matrix for lasso
hotel_x_train = model.matrix(children ~ (.-1 - arrival_date+month(arrival_date))^2, data=hotels_train) # 
hotel_y_train = hotels_train$children
hotel_x_test = model.matrix(children ~ (.-1 - arrival_date+month(arrival_date))^2, data=hotels_test)
hotel_y_test = hotels_test$children
hotel_val_x = model.matrix(children ~ (.-1 - arrival_date+month(arrival_date))^2, data=hotels_val)

hotel_lasso = gamlr(hotel_x_train, hotel_y_train, family="binomial", type = 'response')
y_hat_lasso <- predict(hotel_lasso, newdata = hotel_x_test, type = 'response') %>% as.matrix() %>% as.data.frame()

rmse_lasso <- sqrt(sum((y_hat_lasso - hotel_y_test)^2)/nrow(hotels_test))
rmse_small <- rmse(model_small,hotels_test)
rmse_large <- rmse(model_large,hotels_test)
```


We start off by creating a small model which uses `market_segment`, `adults`, `customer_type`, and `is_repeated_guest` as explanatory variables. We also create a large linear model which includes all variables in our dataset excluding the arrival date. To generate the best possible linear model we utilize the lasso model on all variables and interactions. To measure the out of sample performance of our lasso model, we use the RMSE. For the small, large and lasso models the RMSE's are `r rmse_small`, `r rmse_large`, `r rmse_lasso` respectively. We can see that the lasso model beats the two other models.


```{r echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
phat_test_model_small = predict(model_small, hotels_val, type='response')
phat_test_model_large = predict(model_large, hotels_val, type='response')
phat_test_model_good = predict(hotel_lasso, newdata = hotel_val_x, type='response')
thresh_grid = seq(1, 0, by=-0.005)

roc_curve_hotel = foreach(thresh = thresh_grid, .combine='rbind') %do% {
  yhat_test_model_small = ifelse(phat_test_model_small >= thresh, 1, 0)
  yhat_test_model_large = ifelse(phat_test_model_large >= thresh, 1, 0)
  yhat_test_model_good = ifelse(phat_test_model_good >= thresh, 1, 0)
  confusion_out_model_small = as.matrix(confusionMatrix(data = as.factor(yhat_test_model_small),reference = as.factor(as.matrix(hotels_val$children))))
  confusion_out_model_large = as.matrix(confusionMatrix(data = as.factor(yhat_test_model_large),reference = as.factor(as.matrix(hotels_val$children))))
  confusion_out_model_good =  as.matrix(confusionMatrix(data = as.factor(yhat_test_model_good),reference = as.factor(as.matrix(hotels_val$children))))
  out_model_small = data.frame(model = "Small Linear Model",
                               TPR = confusion_out_model_small[2,2]/sum(hotels_val$children==1),
                               FPR = confusion_out_model_small[2,1]/sum(hotels_val$children==0))
  out_model_large = data.frame(model = "Large Linear Model",
                               TPR = confusion_out_model_large[2,2]/sum(hotels_val$children==1),
                               FPR = confusion_out_model_large[2,1]/sum(hotels_val$children==0))
  out_model_good = data.frame(model = "Lasso Linear Model",
                              TPR = confusion_out_model_good[2,2]/sum(hotels_val$children==1),
                              FPR = confusion_out_model_good[2,1]/sum(hotels_val$children==0))
  rbind(out_model_small, out_model_large,out_model_good)
} %>% as.data.frame()

ggplot(roc_curve_hotel) + 
  geom_line(aes(x=FPR, y=TPR, color=model)) + 
  labs(title="ROC Curves") +
  theme_minimal()

K_folds = 20

hotels_val <- hotels_val %>% 
  mutate(fold_id = rep(1:K_folds, length=nrow(hotels_val)) %>% sample,
y_hat = predict(hotel_lasso, newdata = hotel_val_x,type = 'response'))

compare <- hotels_val %>% 
  group_by(fold_id) %>% 
  summarise(
    pred = sum(y_hat),
    actual = sum(children)
  )

ggplot(compare, aes(x = actual, y=pred))+
  geom_point()+
  geom_abline(slope = 1, intercept = 0)+
  scale_x_continuous(limits = c(12,30))+
  scale_y_continuous(limits = c(12,30))+
  labs(title = "Predicted vs Observed Number of Children",
       caption = "The line represents what a perfect fit would be between
       predicted and observed number of children. We see an 
       upward trend which indicates the model is performing adequetly.")+
  xlab("Observed Number of Children")+
  ylab("Predicted Number of Children")+
  theme_minimal()
```