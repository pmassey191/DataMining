---
title: "Homework 2"
author: "Patrick Massey, Harrison Snell, Brandon Williams"
date: "3/7/2022"
output: md_document
---

```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
library(tidyverse)
library(mosaic)
library(rsample)
library(caret)
library(modelr)
library(parallel)
library(foreach)
library(gamlr)
library(here)
```

## Problem 1
```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

capmetro_UT <- read.csv(here("Data/capmetro_UT.csv"))

# Recode the categorical variables in sensible, rather than alphabetical, order
capmetro_UT = mutate(capmetro_UT,
                     day_of_week = factor(day_of_week,
                                          levels=c("Mon", "Tue", "Wed","Thu", "Fri", "Sat", "Sun")),
                     month = factor(month,
                                    levels=c("Sep", "Oct","Nov")))

fpanel = capmetro_UT %>%
  group_by(hour_of_day,day_of_week,month)%>%
  summarize(avg_board = mean(boarding))

ggplot(fpanel)+
  geom_line(aes(x=hour_of_day,y=avg_board,color=month),size=1.2)+
  facet_wrap(~day_of_week)+
  theme_minimal()+
  ylab("Average Boarding")+
  xlab("Hour of the Day")+
  labs(title= "Boardings by Day of Week, Month, and Hour of Day",
       color = 'Month',
       caption = "The charts generally show that weekdays follow the same pattern of growing boardings throughout the day until a peak around rush hour. The hour of peak boarding tends to stay the same for weekdays, coming in the evening rush hour. However, weekends have a very different ridership profile that is pretty much flat throughout the day. We see that on Mondays in September, ridership is lower, this could stem from Labor Day skewing our average ridership on Mondays in September. Similarly, we also see that later week ridership is lower in November, which could stem from Thanksgiving Break being on Wednesday, Thursday, and Friday in November.")
```

```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
ggplot(capmetro_UT)+
  geom_point(aes(x=temperature,y=boarding,color=weekend),size=.5)+
  facet_wrap(~hour_of_day)+
  xlab("Temperature")+
  ylab("Boardings")+
  labs(title = "Boardings by Weekday/Weekend and Temperature faceted by Hour of Day",
       color = "Weekend",
       caption = "In the figure, we see that ridership is consistently lower on weekends. The early mornig hours tend to have lower ridership with boardings tending to increase throughout the day until the evening. Holding hour of day and weekend/weekday fixed, the temperature does not seem to have much of an impact on boardings. Most likely people are taking the bus for class or work and thus temperature does not change the fact that people still need to get to campus or to their job. ")
```


## Problem 2



## Problem 3
```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
german_credit <- read.csv(here("Data/german_credit.csv"))

prob_default = german_credit%>% 
  group_by(history) %>% 
  summarize(prob = mean(Default))

ggplot(prob_default)+
  geom_col(aes(x=history,y=prob),color='dark green',fill='dark green')+
  theme_minimal()+
  ggtitle("Probability of Default by Credit History")+
  ylab("Probability of Default")+
  xlab("Credit History")

german_split = initial_split(german_credit, prop = 0.8)
german_train = training(german_split)
german_test = testing(german_split)


logit_credit = glm(Default ~ (duration + amount + installment + age + 
                     history + purpose + foreign)^2, data=german_train,
                   family='binomial')

# Confusion Matrix
phat_logit_credit = predict(logit_credit,german_test,
                            type='response')
yhat_logit_credit = ifelse(phat_logit_credit >0.5 , 1 , 0)
confusion_logit = table(y = german_test$Default ,
                        yhat = yhat_logit_credit)


```
Looking at the bar plot of default probability, we see that the highest risk of default falls with those who have "good" credit histories. This is a surprise. We would expect that those with good credit history would pay back their loans, but this points to the problem with the sampling method used. By using retrospective sampling, the bank has collected a sample that overstates the probability of default for those with good credit. In predicting default with the logit model, exponentiating our coefficients correspond to odds of defaults, $e^\lambda$. In looking at the history variable, "terrible" and "poor" both have about a -3 coefficient. This implies that having a terrible or poor credit history decreases the odds of default by about $\frac{1}{e^\lambda}$. This means that good history actually increases the odds of default, an outcome of the flawed data collection. With the problems in the good credit history, the data is not adequate for a predictive model of defaults. Consider two identical people with only credit history differing, our model would say that the person with good credit history is more likely to default and thus a riskier loan. Our experience tells us that this is wrong, we would expect the person with good credit history to be more likely to pay back their loan. Since our predictive model would be poor using this retrospective sampling approach, it would be better to use a random sample of loans. Though we will see a lot of loans that are not defaulted on, it will be better to have a random sample of data to create a predictive model. That way, we have a model that matches our intuition, importantly on the fact that those with good credit history have a low risk of default.

## Problem 4
