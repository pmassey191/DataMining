---
title: "Homework 1"
author: "Patrick Massey, Harrison Snell, Brandon Williams"
date: "2/10/2022"
output: md_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
library(tidyverse)
library(mosaic)
library(airportr)
library(dplyr)
library(usmap)
library(maptools)
library(ggplot2)
library(rgdal)
library(viridis)
library(here)
library(colorspace)
library(lemon)
```


## Problem 1

### Am I actually always late for my connecting flight?

I have taken several flights out of Austin-Bergstrom International Airport. Because there are no direct flights from Austin to Madison, WI, I have to use connecting flights. This prompts the question: why does it always feel like I'm late arriving to my connecting flight?

First, let's look at the average arrival delay by airport. This dataset includes any destination airport with more than 500 flights from ABIA in 2008 (n=26). 

```{r warning=FALSE, message=FALSE}

billboard = read.csv('../data/billboard.csv')
olympics_top20 = read.csv('../data/olympics_top20.csv')
ABIA <- read.csv(here("Data/ABIA.csv"))
knit_print.data.frame <- lemon_print
## Let's take a look at what average delays look like for flights out of Austin 

ABIA_stats = ABIA %>% 
  filter(Origin == 'AUS') %>% 
  group_by(Dest) %>% 
  summarize(count = n(),
            mean_arr_delay = mean(ArrDelay, na.rm=TRUE)) %>% 
  filter(count > 499)

ggplot(ABIA_stats) +
  geom_col(aes(x = mean_arr_delay, fct_reorder(Dest, mean_arr_delay), fill = mean_arr_delay), show.legend = FALSE) +
  scale_fill_continuous_sequential(palette = "Heat"
                                    )  +
  theme_classic() +
  labs(title = "Arrival Delays by Destination Airport",
       subtitle = "Arriving from Austin (At Least 500 Flights)",
       x = "Average Flight Delay (Mins)",
       y = "Destination") +
  geom_vline(aes(xintercept=5.5546)) +
  geom_text(mapping=aes(x=5.5546, y=5, label="Group Average"), size=4, angle=90, vjust=-.3, hjust=.3)

```

Congratulations to Salt Lake City for being early on average! Most airports, however, are late on average (around 5.5 minutes). What does this mean for my connecting flights? According to Business Wire, the major transfer hubs in the US are Dallas/Fort Worth (a major hub for American Airlines), Charlotte (also American), Atlanta (Delta), Chicago-O'Hare (United), and Denver (Frontier). It turns out, if you have to take a connecting flight, Charlotte (CLT) is your best bet, while Denver (DEN) and Dallas (DAL) perform about average for this group, at 5 minutes average arrival delay. Unfortunately for me and many other travellers, the major hubs in Atlanta (ATL) and Chicago (ORD) are in for some of the worst delays in the country for any flight out of Austin. 

```{r echo=FALSE, warning=FALSE, message=FALSE}

## Let's also have only Austin outgoing flights
airports <- airports %>% 
  rename(Dest = IATA)

ABIA_outbound <- ABIA %>% 
  filter(Origin == 'AUS')

## Let's join our tables

ABIA_locations <- inner_join(x = ABIA_outbound, y = airports, by = 'Dest')


## We need to reorganize the data and map it to the usmap package:
ABIA_locations <- ABIA_locations %>%           
  dplyr::select("Latitude", everything()) %>%       # Reorder LAT
  dplyr::select("Longitude", everything()) %>%      # Reorder LONG
  group_by(Longitude, Latitude, Dest) %>%           #Only 100+ flight destinations
  summarize(count = n(),
            mean_arr_delay = mean(ArrDelay, na.rm=TRUE)) %>% 
  filter(count > 499)

ABIA_transformed <- usmap_transform(ABIA_locations)

## Let's select our hubs for labels

ABIA_hubs <- ABIA_transformed %>% 
  filter(Dest == "DEN" |
           Dest == "DAL" |
           Dest == "ORD" |
           Dest == "ATL" |
           Dest == "CLT")

## Plot to our map


plot_usmap() +
  geom_point(data = ABIA_transformed, aes(x = Longitude.1, y = Latitude.1, colour = mean_arr_delay, size = mean_arr_delay),
             alpha = 0.8) +
  scale_color_continuous_sequential(palette = "Heat", guide = "legend", breaks = c(0, 3, 6, 9, 12, 15), limits = c(0, 15)) +
  scale_size_continuous(range = c(1, 20), breaks = c(0, 3, 6, 9, 12, 15), limits = c(0, 15)) +
  ggrepel::geom_label_repel(data = ABIA_hubs,
                            aes(x = Longitude.1, y = Latitude.1, label = Dest),
                            size = 3, alpha = 0.8,
                            label.r = unit(0.5, "lines"), label.size = 0.5,
                            segment.color = "red", segment.size = 1,
                            seed = 1002) +
  labs(title = "Arrival Delays by Destination Airport",
       subtitle = "Arriving from Austin (At Least 500 Flights)"
  ) + 
  theme(legend.position = "right") +
  guides(size=guide_legend(title="Average Delay (Mins)"), color=guide_legend(title="Average Delay (Mins)"))

```

## Problem 2

Part a:

```{r render=lemon_print}
top10 <- billboard %>% 
  group_by(performer, song) %>% 
  summarise(
    count = n()
  ) %>% 
  arrange(desc(count)) %>% 
  head(10)
top10
```

Part b:

```{r}
unique_songs <- billboard %>% 
  filter(year != 1958) %>% 
  filter(year != 2021) %>% 
  group_by(year) %>% 
  distinct(song_id) %>% 
  summarise(
    count = n()
  )
ggplot(unique_songs)+
  geom_line(aes(x = year, y = count),color = "blue")+
  theme_classic()+
  labs(title = "Unique Songs by Year")+
  xlab("Year")+
  ylab("Cound of Unique Songs")+
  theme(plot.title.position = 'plot')
```

Part c:

```{r}
ten_week_hit <- billboard %>% 
  group_by(performer, song) %>% 
  summarise(
    count = n()
  ) %>% 
  arrange(desc(count)) %>% 
  filter(count >= 10) %>% 
  group_by(performer) %>% 
  summarise(count= n()) %>% 
  filter(count >=30)

ggplot(ten_week_hit, aes(fct_rev(fct_reorder(performer,count)),count))+
  geom_bar(stat = "identity")+
  coord_flip()+
  theme_classic()+
  labs(title = "Billboard Top Performers", subtitle = "Number of songs on billboard for at least 10 weeks")+
  ylab("Count of Songs")+
  xlab("Artist")+
  theme(plot.title.position = 'plot')
```

## Problem 3

Part a:

```{r}
top_height <-olympics_top20 %>% 
  filter(sport == 'Athletics') %>%
  group_by(sex) %>% 
  summarise(
    '95th_height' = quantile(height,probs = .95)  
  )
x <- top_height[1,2]
```
The 95th percentile of height for female competitors in the Athletics events is: `r x`

Part b:

```{r}
height_variation <- olympics_top20 %>% 
  filter(sex == 'F') %>% 
  group_by(event) %>% 
  summarise(
    sd = sd(height)
  ) %>% 
  arrange(desc(sd))

```

The event with the highest standard deviation is: `r height_variation[1,1]` which had a standard deviation of `r round(height_variation[1,2],2)`.

Part c:

```{r caption = "We can the from the graph that there is an upward trend in both male and female average age with the age gap shrinking over time."}
swimmers <- olympics_top20 %>% 
  filter(sport == 'Swimming') %>% 
  group_by(year, sex) %>% 
  summarise(
    avg_age = mean(age)
  )
ggplot(data = swimmers, aes(x = year, y = avg_age,color = sex))+
  geom_point()+
  geom_line()+
  theme_classic()+
  xlab("Year")+
  ylab("Average Age")+
  labs(title = "Average Age of Olympic Swimmers", caption = "We can infer from the graph that there is an upward trend in both \n male and female average age with the age gap shrinking over time.")+
  theme(plot.title.position = 'plot')
```

Prior to the 1952 Olympic Games, there appears to be a small number of participants for each year which is causing the large swings early on. After 1952 the amount of participating athletes becomes larger and stabilizes the mean of the age of the participants.

## Problem 4

In this problem, we want to find the optimal value of K in our KNN regressions for each trim. We take the cross validation approach using 5 folds. In the chart of RMSE versus K for each trim there is an average RMSE mapped with one standard error bar. In each case, we use the "1SE" rule to determine our optimal K. That is we chose the largest K value that was within one standard error of the lowest average RMSE observed. Doing this with the 350 trims yields the following results.

```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}
sclass <- read.csv("../data/sclass.csv")

# Cleaning our original data set to just the columns and trims of interest
sclass_clean = sclass %>%
  select(trim, mileage, price)%>%
  filter(trim=='65 AMG' | trim=='350')

# Setting the number of folds and k values to be used in KNN regression
K_folds = 5
k_grid = c(2:30,35,40,45,50,55,60,65,70,80, 90,100)

```


```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}

# Here we want to focus on just the 350
sclass_350 = sclass_clean%>%
  filter(trim=='350')

# Cross validating with the K-Folds approach
sclass350_folds = crossv_kfold(sclass_350, k=K_folds)

cv_grid_350 = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(sclass350_folds$train, ~ knnreg(price ~ mileage, k=k, data = ., use.all=FALSE))
  errs = map2_dbl(models, sclass350_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

# Plotting our RMSE averages versus K
ggplot(cv_grid_350) + 
  geom_point(aes(x=k, y=err))+
  scale_x_log10()+
  geom_errorbar(aes(x=k, ymin = err-std_err, ymax = err+std_err)) +
  labs(title = "350 Trim: RMSE for Different K Values\n", x = "K", y = "RMSE") +
  theme_classic()

# We will now find our optimal K with the 1SE approach
# First we arrange the data set to find the lowest average RMSE
opt_350 = cv_grid_350%>%
  arrange(err)

# This provides us with the 1 standard error allowance and isolates the associated K value
max_err_350 = opt_350[1,2] + opt_350[1,3]
min_k_350 = opt_350[1,1]

# To find the optimal K within 1 standard error, we choose only those K larger than the minimum we found. Then we find the lowest positive difference to determine the optimal K
opt_350 = opt_350%>%
  filter(k >= min_k_350)%>%
  mutate(diff = max_err_350 - err)%>%
  filter(diff > 0)%>%
  arrange(diff)

optk_350 = opt_350[1,1]

# We now want to fit a KNN model to testing data with our optimal K
# Creating a train-test split
sclass350_split = initial_split(sclass_350, prop = 0.8)
sclass350_train = training(sclass350_split)
sclass350_test = testing(sclass350_split)

# Running the KNN regression with optimal K
knn350_optk <- knnreg(price ~ mileage, data=sclass350_train, k=optk_350)

# Creating line of fit
sclass_350_test = sclass350_test %>%
  mutate(price_pred = predict(knn350_optk, sclass350_test))

# Plotting our line of fit
ggplot(data = sclass_350_test) +
  theme_classic()+
  ggtitle(paste("350 Trim: Line of Fit for K=",optk_350, sep = ""))+
  geom_point(mapping = aes(x = mileage, y = price), alpha=0.2) + 
  geom_line(aes(x = mileage, y = price_pred), color='red', size=1.5)


```



From the 350 Trim chart of RMSE versus K, we see that the optimal K is `r optk_350`, as such we fit the KNN model with k=`r optk_350` to the test data as shown above. 

Similarly, we can find an optimal K value for the AMG trim cars.


```{r, echo=FALSE,results=FALSE,warning=FALSE,message=FALSE}


# The same work flow from the 350 trim is followed for the AMG trim
sclass_AMG = sclass_clean%>%
  filter(trim=='65 AMG')

sclassAMG_folds = crossv_kfold(sclass_AMG, k=K_folds)

cv_grid_AMG = foreach(k = k_grid, .combine='rbind') %dopar% {
  models = map(sclassAMG_folds$train, ~ knnreg(price ~ mileage, k=k, data = ., use.all=FALSE))
  errs = map2_dbl(models, sclassAMG_folds$test, modelr::rmse)
  c(k=k, err = mean(errs), std_err = sd(errs)/sqrt(K_folds))
} %>% as.data.frame

ggplot(cv_grid_AMG) + 
  geom_point(aes(x=k, y=err))+
  scale_x_log10()+
  geom_errorbar(aes(x=k, ymin = err-std_err, ymax = err+std_err)) +
  labs(title = "AMG Trim: RMSE for Different K Values\n", x = "K", y = "RMSE") +
  theme_classic()

opt_AMG = cv_grid_AMG%>%
  arrange(err)

max_err_AMG = opt_AMG[1,2] + opt_AMG[1,3]
min_k_AMG = opt_AMG[1,1]

opt_AMG = opt_AMG%>%
  filter(k >= min_k_AMG)%>%
  mutate(diff = max_err_AMG - err)%>%
  filter(diff > 0)%>%
  arrange(diff)

optk_AMG = opt_AMG[1,1]

sclassAMG_split = initial_split(sclass_AMG, prop = 0.8)
sclassAMG_train = training(sclassAMG_split)
sclassAMG_test = testing(sclassAMG_split)

knnAMG_optk <- knnreg(price ~ mileage, data=sclassAMG_train, k=optk_AMG)

sclass_AMG_test = sclassAMG_test %>%
  mutate(price_pred = predict(knnAMG_optk, sclassAMG_test))

ggplot(data = sclass_AMG_test) +
  theme_classic()+
  ggtitle(paste("AMG Trim: Line of Fit for K=",optk_AMG, sep = ""))+
  geom_point(mapping = aes(x = mileage, y = price), alpha=0.2) + 
  geom_line(aes(x = mileage, y = price_pred), color='red', size=1.5)


```


We see that the 1SE approach yields an optimal k=`r optk_AMG`. We observe higher optimal K values for the 350 trim as compared to the AMG. This is due to the fact that the number of observations for the AMG trim is much lower than that of the 350. Thus to earn a lower RMSE, we need a more flexible model, inducing a lower optimal K. Having a lower K will mean that the values in our neighborhood are closer to the original point, which yields a better result for the AMG trim, as the lack of observations implies that a large neighborhood could capture far away points.